# Orquestaci√≥n y Monitorizaci√≥n con Streams y Tasks

El objetivo de este ejercicio es automatizar el flujo de transformaci√≥n de datos usando Streams y Tasks en Snowflake.

Construiremos un pipeline incremental que detecta nuevos pedidos, actualiza tablas intermedias y refresca los agregados autom√°ticamente.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASOS PREVIOS: 

Antes de comenzar, debemos preparar nuestro entorno y crear la base de datos clonada. En primer lugar y para asegurarnos que todos partimos de la misma base, vamos a clonar en nuestra base de datos:

üß† Nota: Reemplaza MY_DB por el nombre que elijas para tu base de datos personal (Por ejemplo: DEV_CURSO_DB_ALUMNO_XX)

```
USE ROLE CURSO_DATA_ENGINEERING;
USE WAREHOUSE WH_CURSO_DATA_ENGINEERING;

-- üëá Sustituye MY_DB por el nombre de tu base de datos
USE DATABASE MY_DB;

-- üëá Crea una base de datos clonada con el nombre que elijas
CREATE OR REPLACE DATABASE MY_DB
CLONE CURSO_DATA_ENGINEERING_TO_BE_CLONED;
```

Ahora crearemos en nuestra Base de Datos y esquema GOLD la siguiente tabla que hace un agregado del n√∫mero de pedidos y su status por fecha. Se alimenta directamente de la tabla SILVER.ORDERS, que ya contiene la informaci√≥n limpia. 

Ejecuta el siguiente comando (recuerda reemplazar MY_DB por el nombre de tu base de datos):

```
CREATE OR REPLACE TABLE MY_DB.GOLD.ORDERS_STATUS_DATE AS (
    SELECT 
        TO_DATE(CREATED_AT) AS FECHA_CREACION_PEDIDO,
        STATUS,
        COUNT(DISTINCT ORDER_ID) AS NUM_PEDIDOS
    FROM 
        MY_DB.SILVER.ORDERS 
    GROUP BY    
        TO_DATE(CREATED_AT),
        STATUS
    ORDER BY 1 DESC
);
```

#### Relaci√≥n entre las capas:

```plaintext
BRONZE (crudo) ‚Üí SILVER (limpio) ‚Üí GOLD (agregado)
‚îÇ                 ‚îÇ                 ‚îÇ
‚îÇ                 ‚îÇ                 ‚îî‚îÄ‚îÄ‚ñ∫ ORDERS_STATUS_DATE (n√∫mero de pedidos por estado y fecha)
‚îÇ                 ‚îî‚îÄ‚îÄ‚ñ∫ ORDERS (datos transformados)
‚îî‚îÄ‚îÄ‚ñ∫ ORDERS_HIST (fuente de pedidos hist√≥ricos)
```

Lo que vamos a hacer en esta pr√°ctica es automatizar este flujo con Streams y Tasks, para que todo el proceso se ejecute sin intervenci√≥n manual.


¬øY si mejor encapsulamos la creaci√≥n de esta tabla en un procedimiento almacenado como aprendimos ayer?, as√≠ podremos recrearla cada vez que la informaci√≥n se actualice:

```
CREATE OR REPLACE PROCEDURE MY_DB.GOLD.update_gold_orders_status()
RETURNS VARCHAR
LANGUAGE SQL
AS
BEGIN

    CREATE OR REPLACE TABLE MY_DB.GOLD.ORDERS_STATUS_DATE AS (
        SELECT 
            TO_DATE(CREATED_AT) AS FECHA_CREACION_PEDIDO,
            STATUS,
            COUNT(DISTINCT ORDER_ID) AS NUM_PEDIDOS
        FROM 
            MY_DB.SILVER.ORDERS
        GROUP BY    
            TO_DATE(CREATED_AT),
            STATUS
        ORDER BY 1 DESC
    );

    return 'Tabla ORDERS_STATUS_DATE actualizada con √©xito';
END;
```

Hasta aqu√≠ los pasos previos, ahora vamos con lo nuevo!

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 1: STREAM

En primer lugar vamos a crear el Stream en nuestro esquema BRONZE (tipo **`APPEND ONLY`**) sobre la tabla **`ORDERS_HIST`**, pero ojo!, lo vamos a crear sobre la tabla **`ORDERS_HIST`** que est√° en la Base de Datos com√∫n! (CURSO_DATA_ENGINEERING_TO_BE_CLONED.BRONZE.ORDERS_HIST), no la que tienes en tu propia Base de Datos.

Lo haremos as√≠ para simular la entrada de nuevos pedidos, es decir, una vez est√© todo configurado insertaremos nosotros a modo de prueba registros y si todo est√° bien configurado, el pipeline que est√°is a punto de construir funcionar√° a la perfecci√≥n!.

Recordad ejecutar siempre desde vuestra Base de Datos y crearemos el Stream con el nombre **`ORDERS_STREAM`**.

Ser√° un Stream sobre la tabla:
```
CURSO_DATA_ENGINEERING_TO_BE_CLONED.BRONZE.ORDERS_HIST;
```

Como siempre, la documentaci√≥n es nuestra amiga:

https://docs.snowflake.com/en/sql-reference/sql/create-stream

üëâ Aseg√∫rate de incluir el APPEND_ONLY = TRUE expl√≠cito

Todo bien? Si lanz√°is SHOW STREAMS lo v√©is?

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 2: TAREA RA√çZ (ROOT TASK)

En este paso vamos a **construir el √°rbol de tareas** que formar√° la base de nuestro **pipeline automatizado**.  El objetivo es que Snowflake ejecute tareas de forma **programada y dependiente**, reaccionando autom√°ticamente cuando haya nuevos datos.

Queremos que gener√©is la tarea ra√≠z (ROOT TASK) ser√° el punto de partida de nuestro flujo de ejecuci√≥n.  

Esta tarea se encargar√° de **detectar nuevos datos en el Stream** y, cuando los haya, **actualizar la tabla `SILVER.ORDERS`** con los registros nuevos o modificados.

### L√≥gica de ejecuci√≥n

Queremos que esta tarea funcione de forma **incremental y autom√°tica**, es decir:

1. **Se ejecutar√° cada 30 segundos** gracias a un *scheduler*.  
2. **Solo se activar√° si el Stream tiene datos nuevos**, usando la funci√≥n:  
   ```sql
   SYSTEM$STREAM_HAS_DATA('ORDERS_STREAM')
   ```

As√≠ pues, vamos a la documentaci√≥n a resfrescar c√≥mo pod√≠amos conseguir esto que os pedimos:

https://docs.snowflake.com/en/sql-reference/sql/create-task

¬øY que es lo que va a ejecutar esta task?, buena pregunta... El objetivo principal es **actualizar la tabla `SILVER.ORDERS`** con los nuevos pedidos o con cambios en los existentes, tomando los datos del **Stream `ORDERS_STREAM`**.

> ‚ö†Ô∏è Es muy importante que uses **tu propia base de datos y esquema**, no la tabla de la base com√∫n.

### Operaci√≥n de Merge

Para lograr esto, utilizaremos un **`MERGE`**, que permite:

- Actualizar registros existentes si coinciden (`WHEN MATCHED THEN UPDATE`)
- Insertar nuevos registros si no existen (`WHEN NOT MATCHED THEN INSERT`)

Documentaci√≥n del Merge --> https://docs.snowflake.com/en/sql-reference/sql/merge

Por tanto, deb√©is completar este bloque de c√≥digo:

```
--MERGE
    MERGE INTO MY_DB.SILVER.ORDERS t
    USING 
    (
        SELECT *
        FROM
            MY_DB.BRONZE.ORDERS_STREAM 
    ) s ON t.ORDER_ID = s.ORDER_ID
            WHEN MATCHED THEN UPDATE ...
```

Como sab√©is para que no haya errores de tipos de datos, debemos castear algunas columnas del Stream antes de insertarlas o actualizar en la capa SILVER. Para que no perd√°is tiempo con los casteos que hic√≠steis ayer en la tabla de **`ORDERS`**...os los dejamos por aqu√≠:

Nota: Tened en cuenta que los nombres de las columnas son iguales en la tabla **`SILVER.ORDERS`** vs **`MY_DB.BRONZE.ORDERS_STREAM`** excepto para las columnas **`PROMO_NAME`** (ORDERS) vs **`PROMO_ID`** (ORDERS_STREAM)

```
ORDER_ID::varchar(100),
SHIPPING_SERVICE::varchar(20),
replace(SHIPPING_COST,',','.')::decimal,
ADDRESS_ID::varchar(50),
CREATED_AT::timestamp_ntz,
IFNULL(promo_id,'N/A'),
ESTIMATED_DELIVERY_AT::timestamp_ntz,
(replace(ORDER_COST,',','.'))::decimal,
USER_ID::varchar(50),
(replace(s.ORDER_TOTAL,',','.'))::decimal,
DELIVERED_AT::timestamp_ntz,
TRACKING_ID::varchar(50),
STATUS::varchar(20),
TIMESTAMPDIFF(HOUR,created_at,delivered_at)
```

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 3: TAREA HIJA

Para probar el concepto que vimos en la teor√≠a de tarea hija en el √°rbol de tareas, crear√©is una nueva tarea en Bronze (**`TASK_HIJA`**) que se lanzar√° justo cuando la tarea ra√≠z (**`ROOT_TASK`**) finalice.

En nuestro caso:

- **Tarea ra√≠z (ROOT_TASK)** ‚Üí actualiza los datos de `SILVER.ORDERS` desde el Stream.
- **Tarea hija (TASK_HIJA)** ‚Üí actualiza los agregados en `GOLD.ORDERS_STATUS_DATE` llamando al procedimiento almacenado.

Para definir que una tarea dependa de otra, usamos la cl√°usula `AFTER` seguida del nombre de la tarea predecesora.   Esto asegura que la tarea hija **solo se ejecute despu√©s de que la tarea ra√≠z haya terminado**, sin importar si la ejecuci√≥n fue exitosa o no (aunque normalmente queremos que sea despu√©s de `SUCCEEDED`).

Documentaci√≥n para este paso --> https://docs.snowflake.com/en/sql-reference/sql/create-task

En este caso la tarea hija ejecutar√° el procedimiento almacenado que hemos creado en las tareas previas:
update_gold_orders_status()

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 4: ACTIVA TUS TAREAS

Una vez creadas las tareas, no olvides activarlas, para ello:

```
ALTER TASK IF EXISTS MY_SCHEMA.TASK_HIJA RESUME;
ALTER TASK IF EXISTS MY_SCHEMA.ROOT_TASK RESUME;
```

¬øPor qu√© se hace `RESUME` primero en la tarea hija?

En Snowflake, **todas las tareas dependientes deben existir y estar activas para que la tarea ra√≠z pueda ejecutar la cadena completa**.  Sin embargo, si activamos primero la ra√≠z y la hija a√∫n no existe o est√° suspendida, la ra√≠z no podr√° disparar correctamente la tarea hija.  

Para comprobar si la tarea ra√≠z (ROOT_TASK) est√° comprobando cada 30 segundos, que es el tiempo que le configuramos, si el Stream tiene datos o no, podemos lanzar una consulta en el task history y comprobarlo:

```
--CHECK TASK HISTORY
SELECT *
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())
WHERE NAME = 'ROOT_TASK';
```

El campo STATE aparecer√° como SCHEDULED y tras cada revisi√≥n, como todav√≠a no hemos insertado datos en el ORDERS_HIST, pasar√° a estado SKIPPED hasta que insertemos datos y entonces deber√≠a ser SUCCEEDED (ojal√°) o FAILED (si algo hay mal).

| Estado      | Significado                                                        |
|------------|-------------------------------------------------------------------|
| `SCHEDULED` | La tarea est√° programada y esperando su turno para ejecutarse.    |
| `SKIPPED`   | La tarea revis√≥ el Stream pero no hab√≠a datos nuevos.             |
| `SUCCEEDED` | La tarea se ejecut√≥ correctamente y realiz√≥ los cambios esperados.|
| `FAILED`    | Hubo un error durante la ejecuci√≥n (revisar logs y SQL).          |

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 5: COMPROBACI√ìN

Ahora nosotros nos vamos a encargar de insertar valores en la tabla de la base de datos com√∫n a la que apunta el Stream que has creado, como si estuvieran entrando y actualiz√°ndose pedidos y si todo va bien, los SPs de las tareas no fallan y la magia del SQL hace su funci√≥n, deber√°s ver como se insertan en la tabla **`ORDERS`** de tu esquema SILVER y como la tabla de GOLD va mostrando los cambios.

```
SELECT * FROM GOLD.ORDERS_STATUS_DATE;
```


#### Diagrama del flujo del pipeline

- `[CURSO_DATA_ENGINEERING_TO_BE_CLONED.BRONZE.ORDERS_HIST]`
  - ‚Üì *(Stream)*
- `[MY_DB.BRONZE.ORDERS_STREAM]`
  - ‚Üì *(Root Task)*
- `[MY_DB.SILVER.ORDERS]`
  - ‚Üì *(Child Task)*
- `[MY_DB.GOLD.ORDERS_STATUS_DATE]`

Si has llegado hasta aqu√≠...enhorabuena!! Has aprendido una parte importante y muy √∫til para la ingesta y transformaci√≥n de la info gracias a la versatilidad que las Streams+Tasks nos aportan en Snowflake.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 6: DESACTIVAR TUS TAREAS Y ELIMINARLAS

Una vez terminada la pr√°ctica...No olvid√©is desactivar vuestras tareas y eliminarlas!, Gracias!!

```
ALTER TASK MY_DB.BRONZE.ROOT_TASK SUSPEND;
ALTER TASK MY_DB.BRONZE.TASK_HIJA SUSPEND;

DROP TASK MY_DB.BRONZE.ROOT_TASK;
DROP TASK MY_DB.BRONZE.TASK_HIJA;
```

Recuerda: al activar las tareas hazlo de abajo hacia arriba (primero la hija y luego la ra√≠z), y al suspenderlas al rev√©s (primero la ra√≠z y luego la hija), para evitar ejecuciones colgantes en el pipeline.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

# Pr√°ctica Extra: Datos Semi-estructurados en Snowflake

Una vez que hemos entendido c√≥mo funcionan los datos semi-estructurados, vamos a aprender a usarlos en Snowflake. ¬øListos? ¬°Vamos!

En esta pr√°ctica vamos a utilizar dos archivos json:
- Simple.json
```
{
    "resultados": [
        {
            "nivel": "101",
            "asignatura": "Franc√©s 1",
            "departamento": "Idiomas"
        },
        {
            "nivel": "111",
            "asignatura": "Trigonometr√≠a",
            "departamento": "Matem√°ticas"
        },
        {
            "nivel": "110",
            "asignatura": "Geometr√≠a",
            "departamento": "Matem√°ticas"
        },
        {
            "nivel": "102",
            "asignatura": "√Ålgebra 2",
            "departamento": "Matem√°ticas"
        },
        {
            "nivel": "101",
            "asignatura": "√Ålgebra 1",
            "departamento": "Matem√°ticas"
        },
        {
            "nivel": "110",
            "asignatura": "F√≠sica",
            "departamento": "Ciencia"
        },
        {
            "nivel": "105",
            "asignatura": "Biolog√≠a",
            "departamento": "Ciencia"
        },
        {
            "nivel": "101",
            "asignatura": "Qu√≠mica",
            "departamento": "Ciencia"
        }
    ]
}
```
- Anidado.json
```
{
    "resultados": [
        {
            "nivel": "101",
            "asignatura": "Franc√©s 1",
            "departamento": "Idiomas",
            "profesor": {
                "nombre": "Claire Dubois",
                "email": "cdubois@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "Parlons Fran√ßais",
                "autor": "Marc Lavoine",
                "edicion": "5ta"
            }
        },
        {
            "nivel": "111",
            "asignatura": "Trigonometr√≠a",
            "departamento": "Matem√°ticas",
            "profesor": {
                "nombre": "Carlos Guti√©rrez",
                "email": "cgutierrez@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "Fundamentos de Trigonometr√≠a",
                "autor": "Isabel S√°nchez",
                "edicion": "3ra"
            }
        },
        {
            "nivel": "110",
            "asignatura": "Geometr√≠a",
            "departamento": "Matem√°ticas",
            "profesor": {
                "nombre": "Laura Jim√©nez",
                "email": "ljimenez@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "Geometr√≠a Descriptiva",
                "autor": "Juan Perez",
                "edicion": "7ma"
            }
        },
        {
            "nivel": "102",
            "asignatura": "√Ålgebra 2",
            "departamento": "Matem√°ticas",
            "profesor": {
                "nombre": "Enrique Salazar",
                "email": "esalazar@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "√Ålgebra Intermedia",
                "autor": "Ana Mar√≠a L√≥pez",
                "edicion": "2da"
            }
        },
        {
            "nivel": "101",
            "asignatura": "√Ålgebra 1",
            "departamento": "Matem√°ticas",
            "profesor": {
                "nombre": "Jos√© Mart√≠n",
                "email": "jmartin@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "Principios de √Ålgebra",
                "autor": "Luis Rodr√≠guez",
                "edicion": "4ta"
            }
        },
        {
            "nivel": "110",
            "asignatura": "F√≠sica",
            "departamento": "Ciencia",
            "profesor": {
                "nombre": "Marta Vidal",
                "email": "mvidal@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "F√≠sica para Estudiantes",
                "autor": "Diana Rivas",
                "edicion": "6ta"
            }
        },
        {
            "nivel": "105",
            "asignatura": "Biolog√≠a",
            "departamento": "Ciencia",
            "profesor": {
                "nombre": "Antonio Banderas",
                "email": "abanderas@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "La Vida en la Tierra",
                "autor": "Patricia Clark",
                "edicion": "8va"
            }
        },
        {
            "nivel": "101",
            "asignatura": "Qu√≠mica",
            "departamento": "Ciencia",
            "profesor": {
                "nombre": "Ricardo Montalb√°n",
                "email": "rmontalban@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "Qu√≠mica General",
                "autor": "Mario Casas",
                "edicion": "9na"
            }
        }
    ]
}
```
Primero creamos un schema llamado SEMIESTRUCTURADOS dentro de nuestra BBDD de BRONZE:
```
USE DATABASE ALUMNOX_BRONZE_DB;
CREATE SCHEMA SEMIESTRUCTURADOS;
USE SCHEMA SEMIESTRUCTURADOS;
```
Para cargar los datos dentro de este schema, vamos a hacerlo mediante la interfaz para hacerlo m√°s r√°pido:

![Untitled (22)](https://github.com/javipo84/Curso_Snowflake/assets/156344357/5af376d6-ad7d-4ed4-be49-97d50c94a993)

Utilizando ‚ÄòBrowse‚Äô seleccionamos el fichero de simple.json y en la parte de abajo, le indicamos el nombre que queremos que tenga esta tabla. En este caso, le pondremos ‚ÄòSIMPLE‚Äô:
![Untitled (23)](https://github.com/javipo84/Curso_Snowflake/assets/156344357/4d2c707a-7e8c-4017-b501-62446977041b)

Ahora seleccionaremos que el DATA TYPE sea un VARIANT:
Le damos a Load y ya tendr√≠amos nuestro fichero SIMPLE. Hacemos lo mismo con el otro fichero llamado anidado.json y le damos a la tabla el nombre de ANIDADO. 

Recuerda ponerlo como tipo VARIANT para poder hace el ejercicio. Si te has olvidado de pon√©rselo, no pasa nada, borra la tabla y vu√©lvela a crear (DROP TABLE ANIDADO; y ¬°listo! vuelta a empezar).

Si hacemos un SELECT de la tabla veremos que nuestros datos ya est√°n cargados:
```
SELECT * FROM SIMPLE;
```
![Untitled (24)](https://github.com/javipo84/Curso_Snowflake/assets/156344357/082c53f8-3b9a-4396-bbd4-c0b1d08167a4)

```
SELECT * FROM ANIDADO;
```
![Untitled (25)](https://github.com/javipo84/Curso_Snowflake/assets/156344357/4f34eb35-3ca2-40d6-9349-9e065c70bc72)

## Trabajar con datos semiestructurados
Ahora que ya tenemos cargadas las tablas, vamos a hacer un par de ejercicios simples para ver nuestros datos.

### 1. Ver nuestros datos
#### a) simple.json
Hemos visto que ya tenemos nuestros datos cargados en las tablas, pero no se pueden ver muy bien porque se quedan con el formato json. Si queremos ver los datos un poquito mejor separ√°ndolos por columnas podemos usar:

```
SELECT
  value:asignatura::STRING AS asignatura,
  value:departamento::STRING AS departamento,
  value:nivel::STRING AS nivel
FROM simple,
LATERAL FLATTEN(input => simple.resultados);
```

La funci√≥n **`[FLATTEN`**](https://docs.snowflake.com/en/sql-reference/functions/flatten) de Snowflake toma un campo semiestructurado y produce una nueva fila para cada elemento si el campo es un array, o para cada campo si es un objeto.

**`LATERAL FLATTEN(input => simple.resultados)`**: Esta parte de la consulta utiliza la funci√≥n **`FLATTEN`** junto con un **`JOIN`** lateral para expandir los datos anidados dentro del JSON.

- **`input => simple.resultados`** especifica que la entrada para la funci√≥n **`FLATTEN`** es el campo **`resultados`** encontrado en el JSON que se encuentra dentro de la tabla **`simple`**.
- **`LATERAL`** permite que cada fila generada por **`FLATTEN`** se una lateralmente con el resultado de tabla **`simple`.**

#### b) anidado.json
Al igual que hemos hecho con simple, podemos hacerlo con anidado. No obstante, ahora en este anidado, dentro de resultados tenemos profesor y dentro de este nombre y email y al igual pasa con libro de texto. Esto lo solucionaremos de la siguiente manera:

```
SELECT
  value:asignatura::STRING AS asignatura,
  value:departamento::STRING AS departamento,
  value:nivel::STRING AS nivel,
  value:profesor.nombre::STRING AS nombre_profesor,
  value:profesor.email::STRING AS email_profesor,
  value:libro_de_texto.titulo::STRING AS titulo_libro,
  value:libro_de_texto.autor::STRING AS autor_libro,
  value:libro_de_texto.edicion::STRING AS edicion_libro
FROM ANIDADO,
LATERAL FLATTEN(input => anidado.resultados);
```

### 2. Ejercicios pr√°cticos

1. Obt√©n todas las asignaturas que pertenecen al departamento de "Matem√°ticas".
```
SELECT
  value:asignatura::STRING AS asignatura
FROM simple,
LATERAL FLATTEN(input => simple.resultados)
WHERE value:departamento::STRING = 'Matem√°ticas';
```
2. ¬øCu√°ntas asignaturas hay en cada nivel educativo? Ordena de manera ascendente por nivel educativo la consulta.
```
SELECT
  value:nivel::STRING AS nivel,
  COUNT(*) AS cantidad_asignaturas
FROM simple,
LATERAL FLATTEN(input => simple.resultados)
GROUP BY value:nivel::STRING
ORDER BY value:nivel::STRING;
```

3. Identifica todas las asignaturas que usan libros de texto en su "5ta" edici√≥n.
```
SELECT
  value:asignatura::STRING AS asignatura,
  value:departamento::STRING AS departamento,
  value:libro_de_texto.titulo::STRING AS titulo_libro,
  value:libro_de_texto.autor::STRING AS autor_libro,
  value:libro_de_texto.edicion::STRING AS edicion_libro
FROM anidado,
LATERAL FLATTEN(input => anidado.resultados)
WHERE value:libro_de_texto.edicion::STRING = '5ta';
```
