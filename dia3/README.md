# OrquestaciÃ³n y MonitorizaciÃ³n con Streams y Tasks

El objetivo de este ejercicio es automatizar el flujo de transformaciÃ³n de datos usando Streams y Tasks en Snowflake.

Construiremos un pipeline incremental que detecta nuevos pedidos, actualiza tablas intermedias y refresca los agregados automÃ¡ticamente.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASOS PREVIOS: 

Antes de comenzar, debemos preparar nuestro entorno y crear la base de datos clonada. En primer lugar y para asegurarnos que todos partimos de la misma base, vamos a clonar en nuestra base de datos:

ðŸ§  Nota: Reemplaza MY_DB por el nombre que elijas para tu base de datos personal (Por ejemplo: DEV_CURSO_DB_ALUMNO_XX)

```
USE ROLE CURSO_DATA_ENGINEERING;
USE WAREHOUSE WH_CURSO_DATA_ENGINEERING;

-- ðŸ‘‡ Sustituye MY_DB por el nombre de tu base de datos
USE DATABASE MY_DB;

-- ðŸ‘‡ Crea una base de datos clonada con el nombre que elijas
CREATE OR REPLACE DATABASE MY_DB
CLONE CURSO_DATA_ENGINEERING_TO_BE_CLONED;
```

Ahora crearemos en nuestra Base de Datos y esquema GOLD la siguiente tabla que hace un agregado del nÃºmero de pedidos y su status por fecha. Se alimenta directamente de la tabla SILVER.ORDERS, que ya contiene la informaciÃ³n limpia. 

Ejecuta el siguiente comando (recuerda reemplazar MY_DB por el nombre de tu base de datos):

```
CREATE OR REPLACE TABLE MY_DB.GOLD.ORDERS_STATUS_DATE AS (
    SELECT 
        TO_DATE(CREATED_AT) AS FECHA_CREACION_PEDIDO,
        STATUS,
        COUNT(DISTINCT ORDER_ID) AS NUM_PEDIDOS
    FROM 
        MY_DB.SILVER.ORDERS 
    GROUP BY    
        TO_DATE(CREATED_AT),
        STATUS
    ORDER BY 1 DESC
);
```

#### RelaciÃ³n entre las capas:

```plaintext
BRONZE (crudo) â†’ SILVER (limpio) â†’ GOLD (agregado)
â”‚                 â”‚                 â”‚
â”‚                 â”‚                 â””â”€â”€â–º ORDERS_STATUS_DATE (nÃºmero de pedidos por estado y fecha)
â”‚                 â””â”€â”€â–º ORDERS (datos transformados)
â””â”€â”€â–º ORDERS_HIST (fuente de pedidos histÃ³ricos)
```

Lo que vamos a hacer en esta prÃ¡ctica es automatizar este flujo con Streams y Tasks, para que todo el proceso se ejecute sin intervenciÃ³n manual.


Â¿Y si mejor encapsulamos la creaciÃ³n de esta tabla en un procedimiento almacenado como aprendimos ayer?, asÃ­ podremos recrearla cada vez que la informaciÃ³n se actualice:

```
CREATE OR REPLACE PROCEDURE MY_DB.GOLD.update_gold_orders_status()
RETURNS VARCHAR
LANGUAGE SQL
AS
BEGIN

    CREATE OR REPLACE TABLE MY_DB.GOLD.ORDERS_STATUS_DATE AS (
        SELECT 
            TO_DATE(CREATED_AT) AS FECHA_CREACION_PEDIDO,
            STATUS,
            COUNT(DISTINCT ORDER_ID) AS NUM_PEDIDOS
        FROM 
            MY_DB.SILVER.ORDERS
        GROUP BY    
            TO_DATE(CREATED_AT),
            STATUS
        ORDER BY 1 DESC
    );

    return 'Tabla ORDERS_STATUS_DATE actualizada con Ã©xito';
END;
```

Hasta aquÃ­ los pasos previos, ahora vamos con lo nuevo!

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 1: STREAM

En primer lugar vamos a crear el Stream en nuestro esquema BRONZE (tipo **`APPEND ONLY`**) sobre la tabla **`ORDERS_HIST`**, pero ojo!, lo vamos a crear sobre la tabla **`ORDERS_HIST`** que estÃ¡ en la Base de Datos comÃºn! (CURSO_DATA_ENGINEERING_TO_BE_CLONED.BRONZE.ORDERS_HIST), no la que tienes en tu propia Base de Datos.

Lo haremos asÃ­ para simular la entrada de nuevos pedidos, es decir, una vez estÃ© todo configurado insertaremos nosotros a modo de prueba registros y si todo estÃ¡ bien configurado, el pipeline que estÃ¡is a punto de construir funcionarÃ¡ a la perfecciÃ³n!.

Recordad ejecutar siempre desde vuestra Base de Datos y crearemos el Stream con el nombre **`ORDERS_STREAM`**.

SerÃ¡ un Stream sobre la tabla:
```
CURSO_DATA_ENGINEERING_TO_BE_CLONED.BRONZE.ORDERS_HIST;
```

Como siempre, la documentaciÃ³n es nuestra amiga:

https://docs.snowflake.com/en/sql-reference/sql/create-stream

ðŸ‘‰ AsegÃºrate de incluir el APPEND_ONLY = TRUE explÃ­cito

Todo bien? Si lanzÃ¡is SHOW STREAMS lo vÃ©is?

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 2: TAREA RAÃZ (ROOT TASK)

En este paso vamos a **construir el Ã¡rbol de tareas** que formarÃ¡ la base de nuestro **pipeline automatizado**.  El objetivo es que Snowflake ejecute tareas de forma **programada y dependiente**, reaccionando automÃ¡ticamente cuando haya nuevos datos.

Queremos que generÃ©is la tarea raÃ­z (ROOT TASK) serÃ¡ el punto de partida de nuestro flujo de ejecuciÃ³n.  

Esta tarea se encargarÃ¡ de **detectar nuevos datos en el Stream** y, cuando los haya, **actualizar la tabla `SILVER.ORDERS`** con los registros nuevos o modificados.

### LÃ³gica de ejecuciÃ³n

Queremos que esta tarea funcione de forma **incremental y automÃ¡tica**, es decir:

1. **Se ejecutarÃ¡ cada 30 segundos** gracias a un *scheduler*.  
2. **Solo se activarÃ¡ si el Stream tiene datos nuevos**, usando la funciÃ³n:  
   ```sql
   SYSTEM$STREAM_HAS_DATA('ORDERS_STREAM')
   ```

AsÃ­ pues, vamos a la documentaciÃ³n a resfrescar cÃ³mo podÃ­amos conseguir esto que os pedimos:

https://docs.snowflake.com/en/sql-reference/sql/create-task

Â¿Y que es lo que va a ejecutar esta task?, buena pregunta... El objetivo principal es **actualizar la tabla `SILVER.ORDERS`** con los nuevos pedidos o con cambios en los existentes, tomando los datos del **Stream `ORDERS_STREAM`**.

> âš ï¸ Es muy importante que uses **tu propia base de datos y esquema**, no la tabla de la base comÃºn.

### OperaciÃ³n de Merge

Para lograr esto, utilizaremos un **`MERGE`**, que permite:

- Actualizar registros existentes si coinciden (`WHEN MATCHED THEN UPDATE`)
- Insertar nuevos registros si no existen (`WHEN NOT MATCHED THEN INSERT`)

DocumentaciÃ³n del Merge --> https://docs.snowflake.com/en/sql-reference/sql/merge

Por tanto, debÃ©is completar este bloque de cÃ³digo:

```
--MERGE
    MERGE INTO MY_DB.SILVER.ORDERS t
    USING 
    (
        SELECT *
        FROM
            MY_DB.BRONZE.ORDERS_STREAM 
    ) s ON t.ORDER_ID = s.ORDER_ID
            WHEN MATCHED THEN UPDATE ...
```

Como sabÃ©is para que no haya errores de tipos de datos, debemos castear algunas columnas del Stream antes de insertarlas o actualizar en la capa SILVER. Para que no perdÃ¡is tiempo con los casteos que hicÃ­steis ayer en la tabla de **`ORDERS`**...os los dejamos por aquÃ­:

Nota: Tened en cuenta que los nombres de las columnas son iguales en la tabla **`SILVER.ORDERS`** vs **`MY_DB.BRONZE.ORDERS_STREAM`** excepto para las columnas **`PROMO_NAME`** (ORDERS) vs **`PROMO_ID`** (ORDERS_STREAM)

```
ORDER_ID::varchar(100),
SHIPPING_SERVICE::varchar(20),
replace(SHIPPING_COST,',','.')::decimal,
ADDRESS_ID::varchar(50),
CREATED_AT::timestamp_ntz,
IFNULL(promo_id,'N/A'),
ESTIMATED_DELIVERY_AT::timestamp_ntz,
(replace(ORDER_COST,',','.'))::decimal,
USER_ID::varchar(50),
(replace(s.ORDER_TOTAL,',','.'))::decimal,
DELIVERED_AT::timestamp_ntz,
TRACKING_ID::varchar(50),
STATUS::varchar(20),
TIMESTAMPDIFF(HOUR,created_at,delivered_at)
```

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 3: TAREA HIJA

Para probar el concepto que vimos en la teorÃ­a de tarea hija en el Ã¡rbol de tareas, crearÃ©is una nueva tarea en Bronze (**`TASK_HIJA`**) que se lanzarÃ¡ justo cuando la tarea raÃ­z (**`ROOT_TASK`**) finalice.

En nuestro caso:

- **Tarea raÃ­z (ROOT_TASK)** â†’ actualiza los datos de `SILVER.ORDERS` desde el Stream.
- **Tarea hija (TASK_HIJA)** â†’ actualiza los agregados en `GOLD.ORDERS_STATUS_DATE` llamando al procedimiento almacenado.

Para definir que una tarea dependa de otra, usamos la clÃ¡usula `AFTER` seguida del nombre de la tarea predecesora.   Esto asegura que la tarea hija **solo se ejecute despuÃ©s de que la tarea raÃ­z haya terminado**, sin importar si la ejecuciÃ³n fue exitosa o no (aunque normalmente queremos que sea despuÃ©s de `SUCCEEDED`).

DocumentaciÃ³n para este paso --> https://docs.snowflake.com/en/sql-reference/sql/create-task

En este caso la tarea hija ejecutarÃ¡ el procedimiento almacenado que hemos creado en las tareas previas:
update_gold_orders_status()

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 4: ACTIVA TUS TAREAS

Una vez creadas las tareas, no olvides activarlas, para ello:

```
ALTER TASK IF EXISTS MY_SCHEMA.TASK_HIJA RESUME;
ALTER TASK IF EXISTS MY_SCHEMA.ROOT_TASK RESUME;
```

Â¿Por quÃ© se hace `RESUME` primero en la tarea hija?

En Snowflake, **todas las tareas dependientes deben existir y estar activas para que la tarea raÃ­z pueda ejecutar la cadena completa**.  Sin embargo, si activamos primero la raÃ­z y la hija aÃºn no existe o estÃ¡ suspendida, la raÃ­z no podrÃ¡ disparar correctamente la tarea hija.  

Para comprobar si la tarea raÃ­z (ROOT_TASK) estÃ¡ comprobando cada 30 segundos, que es el tiempo que le configuramos, si el Stream tiene datos o no, podemos lanzar una consulta en el task history y comprobarlo:

```
--CHECK TASK HISTORY
SELECT *
FROM TABLE(INFORMATION_SCHEMA.TASK_HISTORY())
WHERE NAME = 'ROOT_TASK';
```

El campo STATE aparecerÃ¡ como SCHEDULED y tras cada revisiÃ³n, como todavÃ­a no hemos insertado datos en el ORDERS_HIST, pasarÃ¡ a estado SKIPPED hasta que insertemos datos y entonces deberÃ­a ser SUCCEEDED (ojalÃ¡) o FAILED (si algo hay mal).

| Estado      | Significado                                                        |
|------------|-------------------------------------------------------------------|
| `SCHEDULED` | La tarea estÃ¡ programada y esperando su turno para ejecutarse.    |
| `SKIPPED`   | La tarea revisÃ³ el Stream pero no habÃ­a datos nuevos.             |
| `SUCCEEDED` | La tarea se ejecutÃ³ correctamente y realizÃ³ los cambios esperados.|
| `FAILED`    | Hubo un error durante la ejecuciÃ³n (revisar logs y SQL).          |

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 5: COMPROBACIÃ“N

Ahora nosotros nos vamos a encargar de insertar valores en la tabla de la base de datos comÃºn a la que apunta el Stream que has creado, como si estuvieran entrando y actualizÃ¡ndose pedidos y si todo va bien, los SPs de las tareas no fallan y la magia del SQL hace su funciÃ³n, deberÃ¡s ver como se insertan en la tabla **`ORDERS`** de tu esquema SILVER y como la tabla de GOLD va mostrando los cambios.

```
SELECT * FROM GOLD.ORDERS_STATUS_DATE;
```


#### Diagrama del flujo del pipeline

- `[CURSO_DATA_ENGINEERING_TO_BE_CLONED.BRONZE.ORDERS_HIST]`
  - â†“ *(Stream)*
- `[MY_DB.BRONZE.ORDERS_STREAM]`
  - â†“ *(Root Task)*
- `[MY_DB.SILVER.ORDERS]`
  - â†“ *(Child Task)*
- `[MY_DB.GOLD.ORDERS_STATUS_DATE]`

Si has llegado hasta aquÃ­...enhorabuena!! Has aprendido una parte importante y muy Ãºtil para la ingesta y transformaciÃ³n de la info gracias a la versatilidad que las Streams+Tasks nos aportan en Snowflake.

-------------------------------------------------------------------------------------------------------------------------------------------------------------------

### PASO 6: DESACTIVAR TUS TAREAS Y ELIMINARLAS

Una vez terminada la prÃ¡ctica...No olvidÃ©is desactivar vuestras tareas y eliminarlas!, Gracias!!

```
ALTER TASK MY_DB.BRONZE.ROOT_TASK SUSPEND;
ALTER TASK MY_DB.BRONZE.TASK_HIJA SUSPEND;

DROP TASK MY_DB.BRONZE.ROOT_TASK;
DROP TASK MY_DB.BRONZE.TASK_HIJA;
```

Recuerda: al activar las tareas hazlo de abajo hacia arriba (primero la hija y luego la raÃ­z), y al suspenderlas al revÃ©s (primero la raÃ­z y luego la hija), para evitar ejecuciones colgantes en el pipeline.

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

--------------------------------------------------------------------------------------------------------------------------------------------------------------------

# PrÃ¡ctica Extra: Datos Semi-estructurados en Snowflake

Una vez que hemos entendido cÃ³mo funcionan los datos semi-estructurados, vamos a aprender a usarlos en Snowflake. Â¿Listos? Â¡Vamos!

En esta prÃ¡ctica vamos a utilizar dos archivos json:
- Simple.json
```
{
    "resultados": [
        {
            "nivel": "101",
            "asignatura": "FrancÃ©s 1",
            "departamento": "Idiomas"
        },
        {
            "nivel": "111",
            "asignatura": "TrigonometrÃ­a",
            "departamento": "MatemÃ¡ticas"
        },
        {
            "nivel": "110",
            "asignatura": "GeometrÃ­a",
            "departamento": "MatemÃ¡ticas"
        },
        {
            "nivel": "102",
            "asignatura": "Ãlgebra 2",
            "departamento": "MatemÃ¡ticas"
        },
        {
            "nivel": "101",
            "asignatura": "Ãlgebra 1",
            "departamento": "MatemÃ¡ticas"
        },
        {
            "nivel": "110",
            "asignatura": "FÃ­sica",
            "departamento": "Ciencia"
        },
        {
            "nivel": "105",
            "asignatura": "BiologÃ­a",
            "departamento": "Ciencia"
        },
        {
            "nivel": "101",
            "asignatura": "QuÃ­mica",
            "departamento": "Ciencia"
        }
    ]
}
```
- Anidado.json
```
{
    "resultados": [
        {
            "nivel": "101",
            "asignatura": "FrancÃ©s 1",
            "departamento": "Idiomas",
            "profesor": {
                "nombre": "Claire Dubois",
                "email": "cdubois@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "Parlons FranÃ§ais",
                "autor": "Marc Lavoine",
                "edicion": "5ta"
            }
        },
        {
            "nivel": "111",
            "asignatura": "TrigonometrÃ­a",
            "departamento": "MatemÃ¡ticas",
            "profesor": {
                "nombre": "Carlos GutiÃ©rrez",
                "email": "cgutierrez@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "Fundamentos de TrigonometrÃ­a",
                "autor": "Isabel SÃ¡nchez",
                "edicion": "3ra"
            }
        },
        {
            "nivel": "110",
            "asignatura": "GeometrÃ­a",
            "departamento": "MatemÃ¡ticas",
            "profesor": {
                "nombre": "Laura JimÃ©nez",
                "email": "ljimenez@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "GeometrÃ­a Descriptiva",
                "autor": "Juan Perez",
                "edicion": "7ma"
            }
        },
        {
            "nivel": "102",
            "asignatura": "Ãlgebra 2",
            "departamento": "MatemÃ¡ticas",
            "profesor": {
                "nombre": "Enrique Salazar",
                "email": "esalazar@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "Ãlgebra Intermedia",
                "autor": "Ana MarÃ­a LÃ³pez",
                "edicion": "2da"
            }
        },
        {
            "nivel": "101",
            "asignatura": "Ãlgebra 1",
            "departamento": "MatemÃ¡ticas",
            "profesor": {
                "nombre": "JosÃ© MartÃ­n",
                "email": "jmartin@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "Principios de Ãlgebra",
                "autor": "Luis RodrÃ­guez",
                "edicion": "4ta"
            }
        },
        {
            "nivel": "110",
            "asignatura": "FÃ­sica",
            "departamento": "Ciencia",
            "profesor": {
                "nombre": "Marta Vidal",
                "email": "mvidal@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "FÃ­sica para Estudiantes",
                "autor": "Diana Rivas",
                "edicion": "6ta"
            }
        },
        {
            "nivel": "105",
            "asignatura": "BiologÃ­a",
            "departamento": "Ciencia",
            "profesor": {
                "nombre": "Antonio Banderas",
                "email": "abanderas@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "La Vida en la Tierra",
                "autor": "Patricia Clark",
                "edicion": "8va"
            }
        },
        {
            "nivel": "101",
            "asignatura": "QuÃ­mica",
            "departamento": "Ciencia",
            "profesor": {
                "nombre": "Ricardo MontalbÃ¡n",
                "email": "rmontalban@instituto.edu"
            },
            "libro_de_texto": {
                "titulo": "QuÃ­mica General",
                "autor": "Mario Casas",
                "edicion": "9na"
            }
        }
    ]
}
```
Primero creamos un schema llamado SEMIESTRUCTURADOS dentro de nuestra BBDD de BRONZE:
```
USE DATABASE ALUMNOX_BRONZE_DB;
CREATE SCHEMA SEMIESTRUCTURADOS;
USE SCHEMA SEMIESTRUCTURADOS;
```
Para cargar los datos dentro de este schema, vamos a hacerlo mediante la interfaz para hacerlo mÃ¡s rÃ¡pido:

![Untitled (22)](https://github.com/javipo84/Curso_Snowflake/assets/156344357/5af376d6-ad7d-4ed4-be49-97d50c94a993)

Utilizando â€˜Browseâ€™ seleccionamos el fichero de simple.json y en la parte de abajo, le indicamos el nombre que queremos que tenga esta tabla. En este caso, le pondremos â€˜SIMPLEâ€™:
![Untitled (23)](https://github.com/javipo84/Curso_Snowflake/assets/156344357/4d2c707a-7e8c-4017-b501-62446977041b)

Ahora seleccionaremos que el DATA TYPE sea un VARIANT:
Le damos a Load y ya tendrÃ­amos nuestro fichero SIMPLE. Hacemos lo mismo con el otro fichero llamado anidado.json y le damos a la tabla el nombre de ANIDADO. 

Recuerda ponerlo como tipo VARIANT para poder hace el ejercicio. Si te has olvidado de ponÃ©rselo, no pasa nada, borra la tabla y vuÃ©lvela a crear (DROP TABLE ANIDADO; y Â¡listo! vuelta a empezar).

Si hacemos un SELECT de la tabla veremos que nuestros datos ya estÃ¡n cargados:
```
SELECT * FROM SIMPLE;
```
![Untitled (24)](https://github.com/javipo84/Curso_Snowflake/assets/156344357/082c53f8-3b9a-4396-bbd4-c0b1d08167a4)

```
SELECT * FROM ANIDADO;
```
![Untitled (25)](https://github.com/javipo84/Curso_Snowflake/assets/156344357/4f34eb35-3ca2-40d6-9349-9e065c70bc72)

## Trabajar con datos semiestructurados
Ahora que ya tenemos cargadas las tablas, vamos a hacer un par de ejercicios simples para ver nuestros datos.

### 1. Ver nuestros datos
#### a) simple.json
Hemos visto que ya tenemos nuestros datos cargados en las tablas, pero no se pueden ver muy bien porque se quedan con el formato json. Si queremos ver los datos un poquito mejor separÃ¡ndolos por columnas podemos usar:

```
SELECT
  value:asignatura::STRING AS asignatura,
  value:departamento::STRING AS departamento,
  value:nivel::STRING AS nivel
FROM simple,
LATERAL FLATTEN(input => simple.resultados);
```

La funciÃ³n **`[FLATTEN`**](https://docs.snowflake.com/en/sql-reference/functions/flatten) de Snowflake toma un campo semiestructurado y produce una nueva fila para cada elemento si el campo es un array, o para cada campo si es un objeto.

**`LATERAL FLATTEN(input => simple.resultados)`**: Esta parte de la consulta utiliza la funciÃ³n **`FLATTEN`** junto con un **`JOIN`** lateral para expandir los datos anidados dentro del JSON.

- **`input => simple.resultados`** especifica que la entrada para la funciÃ³n **`FLATTEN`** es el campo **`resultados`** encontrado en el JSON que se encuentra dentro de la tabla **`simple`**.
- **`LATERAL`** permite que cada fila generada por **`FLATTEN`** se una lateralmente con el resultado de tabla **`simple`.**

#### b) anidado.json
Al igual que hemos hecho con simple, podemos hacerlo con anidado. No obstante, ahora en este anidado, dentro de resultados tenemos profesor y dentro de este nombre y email y al igual pasa con libro de texto. Esto lo solucionaremos de la siguiente manera:

```
SELECT
  value:asignatura::STRING AS asignatura,
  value:departamento::STRING AS departamento,
  value:nivel::STRING AS nivel,
  value:profesor.nombre::STRING AS nombre_profesor,
  value:profesor.email::STRING AS email_profesor,
  value:libro_de_texto.titulo::STRING AS titulo_libro,
  value:libro_de_texto.autor::STRING AS autor_libro,
  value:libro_de_texto.edicion::STRING AS edicion_libro
FROM ANIDADO,
LATERAL FLATTEN(input => anidado.resultados);
```

### 2. Ejercicios prÃ¡cticos

1. ObtÃ©n todas las asignaturas que pertenecen al departamento de "MatemÃ¡ticas".
```
SELECT
  value:asignatura::STRING AS asignatura
FROM simple,
LATERAL FLATTEN(input => simple.resultados)
WHERE value:departamento::STRING = 'MatemÃ¡ticas';
```
2. Â¿CuÃ¡ntas asignaturas hay en cada nivel educativo? Ordena de manera ascendente por nivel educativo la consulta.
```
SELECT
  value:nivel::STRING AS nivel,
  COUNT(*) AS cantidad_asignaturas
FROM simple,
LATERAL FLATTEN(input => simple.resultados)
GROUP BY value:nivel::STRING
ORDER BY value:nivel::STRING;
```

3. Identifica todas las asignaturas que usan libros de texto en su "5ta" ediciÃ³n.
```
SELECT
  value:asignatura::STRING AS asignatura,
  value:departamento::STRING AS departamento,
  value:libro_de_texto.titulo::STRING AS titulo_libro,
  value:libro_de_texto.autor::STRING AS autor_libro,
  value:libro_de_texto.edicion::STRING AS edicion_libro
FROM anidado,
LATERAL FLATTEN(input => anidado.resultados)
WHERE value:libro_de_texto.edicion::STRING = '5ta';
```
